{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcaefaad",
   "metadata": {},
   "source": [
    "# Use SVFAP conda env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e9f8f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import torchvision\n",
    "from pipelines.data.data_module import AVSRDataLoader\n",
    "from pipelines.detectors.mediapipe.detector import LandmarksDetector\n",
    "from fractions import Fraction\n",
    "\n",
    "def save2vid(filename, vid, frames_per_second):\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    # fps = float(frames_per_second)\n",
    "    fps = Fraction(frames_per_second).limit_denominator()\n",
    "    torchvision.io.write_video(filename, vid, fps)\n",
    "\n",
    "def preprocess_video(src_filename, dst_filename):\n",
    "    landmarks = landmarks_detector(src_filename)\n",
    "    data = dataloader.load_data(src_filename, landmarks)\n",
    "    \n",
    "    fps_raw = cv2.VideoCapture(src_filename).get(cv2.CAP_PROP_FPS)\n",
    "    fps = float(fps_raw) if fps_raw is not None else 25.0  # Default fallback\n",
    "    print(\"FPS:\", fps, \"Type:\", type(fps))  # Debugging\n",
    "    \n",
    "    save2vid(dst_filename, data, fps)\n",
    "\n",
    "\n",
    "dataloader = AVSRDataLoader(modality=\"video\", speed_rate=1, transform=False, detector=\"mediapipe\", convert_gray=False)\n",
    "landmarks_detector = LandmarksDetector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3feef618",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1746553848.020871  887977 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1746553848.039631  887984 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pipelines.model import AVSR\n",
    "\n",
    "class InferencePipeline(torch.nn.Module):\n",
    "    def __init__(self, modality, model_path, model_conf, detector=\"mediapipe\", face_track=False, device=\"cuda:0\"):\n",
    "        super(InferencePipeline, self).__init__()\n",
    "        self.device = device\n",
    "        # modality configuration\n",
    "        self.modality = modality\n",
    "        self.dataloader = AVSRDataLoader(modality, detector=detector)\n",
    "        self.model = AVSR(modality, model_path, model_conf, rnnlm=None, rnnlm_conf=None, penalty=0.0, ctc_weight=0.1, lm_weight=0.0, beam_size=40, device=device)\n",
    "        if face_track and self.modality in [\"video\", \"audiovisual\"]:\n",
    "            self.landmarks_detector = LandmarksDetector()\n",
    "        else:\n",
    "            self.landmarks_detector = None\n",
    "\n",
    "\n",
    "    def process_landmarks(self, data_filename, landmarks_filename):\n",
    "        if self.modality == \"audio\":\n",
    "            return None\n",
    "        if self.modality in [\"video\", \"audiovisual\"]:\n",
    "            landmarks = self.landmarks_detector(data_filename)\n",
    "            return landmarks\n",
    "\n",
    "\n",
    "    def forward(self, data_filename, landmarks_filename=None):\n",
    "        assert os.path.isfile(data_filename), f\"data_filename: {data_filename} does not exist.\"\n",
    "        landmarks = self.process_landmarks(data_filename, landmarks_filename)\n",
    "        data = self.dataloader.load_data(data_filename, landmarks)\n",
    "        transcript = self.model.infer(data)\n",
    "        return transcript\n",
    "\n",
    "    def extract_features(self, data_filename, landmarks_filename=None, extract_resnet_feats=False):\n",
    "        assert os.path.isfile(data_filename), f\"data_filename: {data_filename} does not exist.\"\n",
    "        landmarks = self.process_landmarks(data_filename, landmarks_filename)\n",
    "        data = self.dataloader.load_data(data_filename, landmarks)\n",
    "        with torch.no_grad():\n",
    "            if isinstance(data, tuple):\n",
    "                enc_feats = self.model.model.encode(data[0].to(self.device), data[1].to(self.device), extract_resnet_feats)\n",
    "            else:\n",
    "                enc_feats = self.model.model.encode(data.to(self.device), extract_resnet_feats)\n",
    "        return enc_feats     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5fcfccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPS: 24.0 Type: <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "preprocess_video(src_filename=\"clip.mp4\", dst_filename=\"/clip_roi.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86b5c1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(a_upsample_ratio=1, accum_grad=2, adim=768, aheads=12, apply_uttmvn=True, aux_lsm_weight=0.0, backend='pytorch', badim=320, batch_bins=0, batch_count='auto', batch_frames_in=0, batch_frames_inout=0, batch_frames_out=0, bdropout_rate=0.0, beam_size=4, blayers=2, bnmask=2, bprojs=300, btype='blstmp', bunits=300, cnn_module_kernel=31, config2=None, config3=None, context_residual=False, criterion='acc', ctc_type='warpctc', ctc_weight=0.3, debugmode=1, dec_init=None, dec_init_mods=['att.', ' dec.'], dict='data/lang_1char/units.txt', dlayers=6, dropout_rate=0.1, dunits=3072, early_stop_criterion='validation/main/acc', elayers=12, enc_init=None, enc_init_mods=['enc.enc.'], eps=1e-08, eps_decay=0.01, eunits=3072, fbank_fmax=None, fbank_fmin=0.0, fbank_fs=16000, grad_clip=5.0, grad_noise=False, labels_type='unigram5000', lm_weight=0.1, lsm_weight=0.1, macaron_style=1, maxlen_in=220, maxlen_out=220, maxlenratio=0.0, minibatches=0, minlenratio=0.0, model_module='espnet.nets.pytorch_backend.e2e_asr_transformer_multitask_dual:E2E', mtl_custom_worker_l1_weight=0.0, mtl_custom_worker_length_normalized_loss=0, mtl_custom_worker_mlp_hdim=256, mtl_custom_worker_mlp_nlayers=2, mtl_custom_worker_mlp_nonlin_end=0, mtl_custom_worker_mlp_nonlin_type='relu', mtl_custom_worker_name='patrickvonplaten/wav2vec2-base', mtl_custom_worker_task_type='', mtl_custom_worker_tgt_type='projected_quantized_states', mtl_kl_weight=0.0, mtl_kl_weight_2=0.0, mtl_l1_weight=0.4, mtl_l1_weight_2=0.4, mtl_length_normalized_loss=1, mtl_length_normalized_loss_2=1, mtl_mlp_hdim=256, mtl_mlp_hdim_2=256, mtl_mlp_nlayers=1, mtl_mlp_nlayers_2=1, mtl_mlp_nonlin_end=0, mtl_mlp_nonlin_end_2=0, mtl_mlp_nonlin_type='relu', mtl_mlp_nonlin_type_2='relu', mtl_task_layer='conformer6', mtl_task_type='l1', mtl_task_type_2='l1', mtl_worker_source='conv1d_lrs3_v04_lrs2', mtl_worker_source_2='conv3d_lrs3_v04_lrs2_dual', mtlalpha=0.1, n_iter_processes=12, n_mels=80, nbest=1, ngpu=1, num_encs=1, num_input=2, num_save_attention=3, num_spkrs=1, opt='noam', patience=0, penalty=0.0, preprocess_conf=None, pretrain_dataset='lrs2_full_dual_ignore', raw_max_freq_width=150, raw_max_speed_rate=1.1, raw_max_time_width=0.4, raw_min_speed_rate=0.9, raw_n_freq_mask=2, raw_n_time_mask=2, raw_speech_do_normalize=False, ref_channel=-1, rel_pos_type='latest', relu_type='swish', report_cer=False, report_interval_iters=100, report_wer=False, rnnlm=None, rnnlm_conf=None, save_interval_iters=0, seed=1, sortagrad=0, specaug_max_freq_width=30, specaug_max_time_warp=5, specaug_max_time_width=40, specaug_n_freq_mask=2, specaug_n_time_mask=2, sr_interp_mode='nearest', sr_interp_scale_factor=1.0, stats_file=None, sym_blank='<blank>', sym_space='<space>', threshold=0.0001, train_dtype='float32', transformer_attn_dropout_rate=0.1, transformer_encoder_attn_layer_type='rel_mha', transformer_init='pytorch', transformer_input_layer='conv3d', transformer_length_normalized_loss=0, transformer_warmup_steps=25000, use_beamformer=True, use_cnn_module=1, use_dnn_mask_for_wpe=False, use_freqmask=False, use_frontend=False, use_noiseaug=False, use_specaug=False, use_speedaug=False, use_timemask=False, use_v_adaptive_timemask=True, use_v_cutout=False, use_v_timemask=False, use_wpe=False, uttmvn_norm_means=True, uttmvn_norm_vars=False, v_cutout_max_hole_length=22, v_cutout_n_holes=1, v_raw_max_time_width=0.4, v_raw_n_time_mask=1, v_timemask_replace_with_zero=False, v_timemask_stride=1.0, verbose=0, wavaugments=None, wdropout_rate=0.0, weight_decay=0.0, wlayers=2, wpe_delay=3, wpe_taps=5, wprojs=300, wtype='blstmp', wunits=300, zero_triu=False) 5049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1746553863.902075  888282 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "modality = \"video\"\n",
    "model_conf = \"LRS3_V_WER19.1/model.json\"  \n",
    "model_path = \"LRS3_V_WER19.1/model.pth\"\n",
    "pipeline = InferencePipeline(modality, model_path, model_conf, face_track=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edfdd87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1746553863.921940  888289 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([178, 768])\n"
     ]
    }
   ],
   "source": [
    "features = pipeline.extract_features(\"/clip_roi.mp4\")\n",
    "print(features.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8e0252e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ba6c17d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.10826629,  0.10884785,  0.04951465, ..., -0.2946354 ,\n",
       "        -0.29910976, -0.3329704 ],\n",
       "       [-0.18616872,  0.1710677 , -0.00855133, ..., -0.03903882,\n",
       "        -0.21815234, -0.5626292 ],\n",
       "       [-0.22646238,  0.42673042,  0.02385088, ...,  0.126887  ,\n",
       "        -0.15350495, -0.3092311 ],\n",
       "       ...,\n",
       "       [-0.02050836, -0.00163938,  0.8248512 , ..., -0.01400121,\n",
       "         0.02664605,  0.00908441],\n",
       "       [-0.00380427, -0.00457797,  0.8197377 , ..., -0.01151075,\n",
       "         0.00412359, -0.00309835],\n",
       "       [ 0.00163391,  0.01081297,  0.79678965, ..., -0.00742658,\n",
       "         0.00794817,  0.02704286]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.load(\"/data/audio-video-deepfake-2/ASR_features/LRS3_V_WER19.1/real+fake/val/id00046/9h3XRcuVI0s/00009/fake_video_real_audio.npz\", allow_pickle=True)\n",
    "x['visual']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ASR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
